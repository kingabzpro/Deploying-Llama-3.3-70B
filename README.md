# Deploying-Llama-3.3-70B
Serve Llama 3.3 70B (with AWQ quantization) using vLLM and deploy it on BentoCloud.
